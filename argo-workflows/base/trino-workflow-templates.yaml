apiVersion: argoproj.io/v1alpha1
kind: WorkflowTemplate
metadata:
  name: "trino-workflow-templates"
  labels:
    owner: "dh-argo-workflows"
    app: "trino-admin"
    pipeline: "trino-partition-updater"
spec:
  templates:
    - name: extract-partitioned-table-names
      script:
        image: "quay.io/opendatahub/trino:356"
        env:
          - name: TRINO_USER
            valueFrom:
              secretKeyRef:
                name: trino-admin-credentials
                key: TRINO_USER
          - name: TRINO_PASSWORD
            valueFrom:
              secretKeyRef:
                name: trino-admin-credentials
                key: TRINO_PASSWORD
          - name: TRINO_HOST
            valueFrom:
              secretKeyRef:
                name: trino-admin-credentials
                key: TRINO_HOST
        command: [python]
        source: |
          import trino
          import json
          import urllib3
          import os

          # Temporarily disabling warnings from urllib3
          urllib3.disable_warnings()

          SHOW_CREATE_TABLES="""
          select
              table_schema,
              table_name,
              'show create table '||table_catalog||'.'||table_schema||'.'||table_name
          from
              hive.information_schema.tables
          where
              table_schema <> 'information_schema'
          """

          conn = trino.dbapi.connect(
              host=os.environ["TRINO_HOST"],
              port=443,
              user=os.environ["TRINO_USER"],
              catalog='hive',
              http_scheme='https',
              auth=trino.auth.BasicAuthentication(os.environ["TRINO_USER"], os.environ["TRINO_PASSWORD"]),
              verify=False
          )
          cur = conn.cursor()
          cur.execute(SHOW_CREATE_TABLES)
          rows = cur.fetchall()
          
          schemas = []

          for row in rows:
              create_table_sql = cur.execute(row[2])
              if "partitioned_by" in cur.fetchall()[0][0]:
                  print("Adding {}.{} to the list of partitioned tables...".format(row[0],row[1]))

                  schema = next((item for item in schemas if item["name"] == row[0]), None)


                  if schema:
                      schema["tables"].append(row[1])
                  else:
                      schema = {}
                      schema["name"] = row[0]
                      schema["tables"] = []
                      schema["tables"].append(row[1])
                      schemas.append(schema)

          with open('/mnt/out/tbl_list','w') as f:
              print("Dumping table list as json data...")
              json.dump(schemas, f)

        volumeMounts:
          - name: out
            mountPath: /mnt/out
      volumes:
        - name: out
          emptyDir: {}
      outputs:
        parameters:
          - name: tables_list
            valueFrom:
              path: /mnt/out/tbl_list
       
    - name: update-partition-metadata
      inputs:
        parameters:
          - name: table_defs
      script:
        image: "quay.io/opendatahub/trino:356"
        env:
          - name: TRINO_USER
            valueFrom:
              secretKeyRef:
                name: trino-admin-credentials
                key: TRINO_USER
          - name: TRINO_PASSWORD
            valueFrom:
              secretKeyRef:
                name: trino-admin-credentials
                key: TRINO_PASSWORD
          - name: TRINO_HOST
            valueFrom:
              secretKeyRef:
                name: trino-admin-credentials
                key: TRINO_HOST
        command: [python]
        source: |
          import trino
          import json
          import urllib3
          import os
          import sys

          # Disable warnings from urllib3
          urllib3.disable_warnings()

          TABLE_DEFS = '{{inputs.parameters.table_defs}}'

          UPDATE_PARTITION = "call hive.system.sync_partition_metadata('{}','{}','FULL')"

          conn = trino.dbapi.connect(
              host=os.environ["TRINO_HOST"],
              port=443,
              user=os.environ["TRINO_USER"],
              catalog='hive',
              http_scheme='https',
              auth=trino.auth.BasicAuthentication(os.environ["TRINO_USER"], os.environ["TRINO_PASSWORD"]),
              verify=False
          )

          table_definitions = json.loads(TABLE_DEFS)

          schema_name = table_definitions["name"]
          schema_tables = table_definitions["tables"]

          print("Begin partition metadata update for schema {}...".format(schema_name))
          for table in schema_tables:
              cur = conn.cursor()
              sys.stdout.write("Updating partition metadata update for table {}...".format(table))
              cur.execute(UPDATE_PARTITION.format(schema_name, table))
              cur.fetchone()
              sys.stdout.write("Done!")
              sys.stdout.flush()
